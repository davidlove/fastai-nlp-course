{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying Logistic Regression on Topic Models\n",
    "\n",
    "I'd like to try comparing logistic regression on standard vectorizers against running it on topic models, to see if the less sparse variables help significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/anaconda3/lib/python3.7/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /opt/conda/conda-bld/pytorch_1603729047590/work/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "from fastai import *\n",
    "from fastai.text import *\n",
    "from fastai.utils.mem import GPUMemTrace #call with mtrace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.feature_extraction.text as sklearn_text\n",
    "import pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/david/.fastai/data/imdb')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = untar_data(URLs.IMDB)\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_colwidth = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home/david/.fastai/data/imdb/train/labeledBow.feat'),\n",
       " PosixPath('/home/david/.fastai/data/imdb/train/unsupBow.feat'),\n",
       " PosixPath('/home/david/.fastai/data/imdb/train/neg'),\n",
       " PosixPath('/home/david/.fastai/data/imdb/train/pos')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(path/'train').ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failure count is 0\n",
      "\n",
      "CPU times: user 7.9 s, sys: 1.96 s, total: 9.86 s\n",
      "Wall time: 39.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# throws `BrokenProcessPool' Error sometimes. Keep trying `till it works!\n",
    "count = 0\n",
    "error = True\n",
    "while error:\n",
    "    try: \n",
    "        # Preprocessing steps\n",
    "        reviews_full = (TextList.from_folder(path)\n",
    "             #  Make a `TextList` object that is a list of `WindowsPath` objects, \n",
    "             #     each of which contains the full path to one of the data files.\n",
    "             .split_by_folder(valid='test')\n",
    "             # Generate a `LabelLists` object that splits files by training and validation folders\n",
    "             # Note: .label_from_folder in next line causes the `BrokenProcessPool` error\n",
    "             .label_from_folder(classes=['neg', 'pos']))\n",
    "             # Create a `CategoryLists` object which contains the data and\n",
    "             #   its labels that are derived from folder names\n",
    "        error = False\n",
    "        print(f'failure count is {count}\\n')    \n",
    "    except: # catch *all* exceptions\n",
    "        # accumulate failure count\n",
    "        count = count + 1\n",
    "        print(f'failure count is {count}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = reviews_full.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab.itos) - len(set(vocab.itos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "xxfake          3\n",
       "titanium        1\n",
       "strident        1\n",
       "anachronism     1\n",
       "raimi           1\n",
       "               ..\n",
       "nationals       1\n",
       "disciplining    1\n",
       "mafioso         1\n",
       "marketing       1\n",
       "blades          1\n",
       "Length: 38438, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(vocab.itos).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(38437, 'xxfake'), (38438, 'xxfake'), (38439, 'xxfake')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(idx,s) for idx,s in enumerate(vocab.itos) if s == 'xxfake']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['zizola',\n",
       " 'cornette',\n",
       " 'knobs',\n",
       " 'elster',\n",
       " 'dm',\n",
       " \"o'daniel\",\n",
       " 'lucianna',\n",
       " 'marcie',\n",
       " 'magalh√£es',\n",
       " 'xxfake']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fixes the multiplicity of 'xxfake'\n",
    "\n",
    "vocab.itos = vocab.itos[:-2]\n",
    "vocab.itos[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.stoi['xxfake'] = vocab.itos.index('xxfake')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "any(idx == 38437 for idx in vocab.stoi.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38437"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.stoi['xxfake']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        79719\n",
       "2708         1\n",
       "14994        1\n",
       "8849         1\n",
       "10896        1\n",
       "         ...  \n",
       "13612        1\n",
       "3371         1\n",
       "1322         1\n",
       "7465         1\n",
       "28626        1\n",
       "Length: 38438, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(vocab.stoi).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Will use Fast.ai's creation of the term document matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = sklearn_text.CountVectorizer(vocabulary=vocab.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xxbos i enjoyed the previous xxmaj ittenbach movie that i \\'d seen , \" xxmaj burning xxmaj moon \" . xxmaj but while that movie was rather grim and nasty , \" xxmaj premutos \" seems to mostly play it for laughs . xxmaj while its admirable how xxmaj ittenbach made this movie with no money in his spare time ( and the xxup dvd documentary is worthwhile to see this ) , i found myself constantly battling not to fast - forward to the next gore scene . xxmaj sure , there \\'s gore , and if that \\'s all you want then go ahead and enjoy . xxmaj but be warned : there \\'s an inordinate amount of lame comedy and tedious story exposition . xxmaj many are comparing this to xxmaj peter xxmaj jackson \\'s movies , especially \" xxmaj braindead \" . xxmaj but looking at what xxmaj jackson did on a similar budget in \" xxmaj bad xxmaj taste \" , it \\'s clear xxmaj ittenbach is lacking one thing that xxmaj jackson has - talent . 3 / 10 ( for pretty good and plentiful gore effects , and for getting the most out of limited resources - but not worth the money i paid for it )'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_full.train.x[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.86 s, sys: 58.7 ms, total: 4.92 s\n",
      "Wall time: 4.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "doc_term_mat = cv.fit_transform([x.text for x in reviews_full.train.x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_doc_term = cv.transform([x.text for x in reviews_full.valid.x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<25000x38438 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 3360072 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_doc_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CategoryList (25000 items)\n",
       "neg,neg,neg,neg,neg\n",
       "Path: /home/david/.fastai/data/imdb"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_full.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neg', 'pos']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_full.y.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, ..., 1, 1, 1, 1])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_full.train.y.items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import NMF, TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_only = make_pipeline(LogisticRegression(dual=True, solver='liblinear', max_iter=500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.19 s, sys: 27.8 ms, total: 8.22 s\n",
      "Wall time: 8.24 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.86532"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "lr_only.fit(doc_term_mat, reviews_full.train.y.items)\n",
    "(lr_only.predict(valid_doc_term) == reviews_full.valid.y.items).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NMF is not working well.  Kinda surprising to me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "violation: 1.0\n",
      "violation: 0.3451984883220355\n",
      "violation: 0.3924725757634428\n",
      "violation: 0.36359922386274257\n",
      "violation: 0.3129004257175245\n",
      "violation: 0.2700128556340622\n",
      "violation: 0.22959302727181125\n",
      "violation: 0.1977870342618297\n",
      "violation: 0.17282701295307032\n",
      "violation: 0.1556238192227033\n",
      "violation: 0.1426735384340764\n",
      "violation: 0.13251497083772987\n",
      "violation: 0.12180697048645311\n",
      "violation: 0.10822876366292548\n",
      "violation: 0.09498537728808175\n",
      "violation: 0.08454099229448633\n",
      "violation: 0.07640475418903786\n",
      "violation: 0.068934119622936\n",
      "violation: 0.062244958017542595\n",
      "violation: 0.05606276660804158\n",
      "violation: 0.05103184882304673\n",
      "violation: 0.046385138248229894\n",
      "violation: 0.04236428412331383\n",
      "violation: 0.03896012046218541\n",
      "violation: 0.03618439287896018\n",
      "violation: 0.03383196321334917\n",
      "violation: 0.031847104335207224\n",
      "violation: 0.030171440243737066\n",
      "violation: 0.028455237309270975\n",
      "violation: 0.02719454134737227\n",
      "violation: 0.02617031276665348\n",
      "violation: 0.025251869505587596\n",
      "violation: 0.024420702980483235\n",
      "violation: 0.023618960708617794\n",
      "violation: 0.022915848465980434\n",
      "violation: 0.022290075685056948\n",
      "violation: 0.02167980933570228\n",
      "violation: 0.021073789371015793\n",
      "violation: 0.02049663546227552\n",
      "violation: 0.020011473826390885\n",
      "violation: 0.019489222256146212\n",
      "violation: 0.019023276240233245\n",
      "violation: 0.01858628382591338\n",
      "violation: 0.018172846950316433\n",
      "violation: 0.017780620908613237\n",
      "violation: 0.01740267307770169\n",
      "violation: 0.017030974919035873\n",
      "violation: 0.016686714398850732\n",
      "violation: 0.016361593190411892\n",
      "violation: 0.01601784021789028\n",
      "violation: 0.01572692285631611\n",
      "violation: 0.015455751148786185\n",
      "violation: 0.015237418774189783\n",
      "violation: 0.015007874261937166\n",
      "violation: 0.014804620602199309\n",
      "violation: 0.014591438053482952\n",
      "violation: 0.014381510283468917\n",
      "violation: 0.014181361275989473\n",
      "violation: 0.013987733313755451\n",
      "violation: 0.01381698193471013\n",
      "violation: 0.013661417937819944\n",
      "violation: 0.013544554611387372\n",
      "violation: 0.013377134199948817\n",
      "violation: 0.013230192205696828\n",
      "violation: 0.013114162403772407\n",
      "violation: 0.013008909158637635\n",
      "violation: 0.0128713535904336\n",
      "violation: 0.012757661627008449\n",
      "violation: 0.012652320002882503\n",
      "violation: 0.012535907274704747\n",
      "violation: 0.01243287480562373\n",
      "violation: 0.012314964084429815\n",
      "violation: 0.012199498959752217\n",
      "violation: 0.012104639293713733\n",
      "violation: 0.012009573050285478\n",
      "violation: 0.011911453843055583\n",
      "violation: 0.011810814226756478\n",
      "violation: 0.011735049592332744\n",
      "violation: 0.011643218173265964\n",
      "violation: 0.011555624381390982\n",
      "violation: 0.011477617004643951\n",
      "violation: 0.01140538628868321\n",
      "violation: 0.011332217491932021\n",
      "violation: 0.011257428765125822\n",
      "violation: 0.01120116686038989\n",
      "violation: 0.011099531962021332\n",
      "violation: 0.011014989488836382\n",
      "violation: 0.010962472841232966\n",
      "violation: 0.010867267063031715\n",
      "violation: 0.010775872140220162\n",
      "violation: 0.010668363169251632\n",
      "violation: 0.010586830161502879\n",
      "violation: 0.01047893426797237\n",
      "violation: 0.010368769616590522\n",
      "violation: 0.010286603964056123\n",
      "violation: 0.010203337131117695\n",
      "violation: 0.010129448188467715\n",
      "violation: 0.010047071296475574\n",
      "violation: 0.009910403802862748\n",
      "violation: 0.00974174795657663\n",
      "violation: 0.009560474159948143\n",
      "violation: 0.009348678690221515\n",
      "violation: 0.009117760730601747\n",
      "violation: 0.008845844152804334\n",
      "violation: 0.008563494671679774\n",
      "violation: 0.008287661807954767\n",
      "violation: 0.008007725178363524\n",
      "violation: 0.007747351985841863\n",
      "violation: 0.007509975851135941\n",
      "violation: 0.007286581816627947\n",
      "violation: 0.007084302978956647\n",
      "violation: 0.006901726633795595\n",
      "violation: 0.006737935924516261\n",
      "violation: 0.006590380786171901\n",
      "violation: 0.006454723871919277\n",
      "violation: 0.00633845861914126\n",
      "violation: 0.0062384908104021145\n",
      "violation: 0.006136876849218351\n",
      "violation: 0.006039526557923542\n",
      "violation: 0.005960358045229614\n",
      "violation: 0.005889270227948242\n",
      "violation: 0.0057967796163947575\n",
      "violation: 0.005719556112625645\n",
      "violation: 0.005653154212974049\n",
      "violation: 0.005585354013499786\n",
      "violation: 0.005511980415587409\n",
      "violation: 0.0054544420499582375\n",
      "violation: 0.0053999965373510315\n",
      "violation: 0.00535386553157348\n",
      "violation: 0.005314756237637355\n",
      "violation: 0.005257513789170108\n",
      "violation: 0.00519874457418909\n",
      "violation: 0.005152731756714961\n",
      "violation: 0.005100298840133115\n",
      "violation: 0.00504436596874449\n",
      "violation: 0.004998421002728442\n",
      "violation: 0.0049514388718013965\n",
      "violation: 0.004897747652787527\n",
      "violation: 0.004855341016342701\n",
      "violation: 0.0048106533765909345\n",
      "violation: 0.004770769829779873\n",
      "violation: 0.004731138555118798\n",
      "violation: 0.004692462533466102\n",
      "violation: 0.0046534159060242115\n",
      "violation: 0.0046152323627131515\n",
      "violation: 0.0045790362457287055\n",
      "violation: 0.004548893196806865\n",
      "violation: 0.0045131782293379346\n",
      "violation: 0.00447852283607833\n",
      "violation: 0.0044442021472531216\n",
      "violation: 0.004411922312688285\n",
      "violation: 0.004380319338400191\n",
      "violation: 0.004350264209335004\n",
      "violation: 0.00431432602823631\n",
      "violation: 0.004284426484510871\n",
      "violation: 0.0042547598080145\n",
      "violation: 0.004225404781302936\n",
      "violation: 0.004197571969620283\n",
      "violation: 0.004168574820656042\n",
      "violation: 0.004140824352182574\n",
      "violation: 0.0041137741837117825\n",
      "violation: 0.004087648955212306\n",
      "violation: 0.004061970886599738\n",
      "violation: 0.004041072325919819\n",
      "violation: 0.004017185931428224\n",
      "violation: 0.0039929269727204085\n",
      "violation: 0.00396966842315248\n",
      "violation: 0.003946548076459402\n",
      "violation: 0.003924299467541392\n",
      "violation: 0.0039026115215591776\n",
      "violation: 0.003882007554942317\n",
      "violation: 0.0038616166490559713\n",
      "violation: 0.0038423862351489725\n",
      "violation: 0.0038239350037622714\n",
      "violation: 0.0038055291044934232\n",
      "violation: 0.0037881518184206225\n",
      "violation: 0.0037707048088632808\n",
      "violation: 0.0037546360712927\n",
      "violation: 0.0037395607881243755\n",
      "violation: 0.003725256908403454\n",
      "violation: 0.003701331593936101\n",
      "violation: 0.003686476595592479\n",
      "violation: 0.003669527034316269\n",
      "violation: 0.0036558548912880772\n",
      "violation: 0.0036398562037760268\n",
      "violation: 0.0036258054324666265\n",
      "violation: 0.0036124841821190876\n",
      "violation: 0.0035999441149983987\n",
      "violation: 0.0035886404625939568\n",
      "violation: 0.0035753812989421955\n",
      "violation: 0.003564142819974076\n",
      "violation: 0.0035488349687614416\n",
      "violation: 0.0035370231398378785\n",
      "violation: 0.0035248019725446765\n",
      "violation: 0.0035132981656662613\n",
      "violation: 0.0035023302707881867\n",
      "violation: 0.0034901595227608048\n",
      "violation: 0.0034807417351780437\n",
      "violation: 0.003469626575653692\n",
      "violation: 0.0034588970477792743\n",
      "violation: 1.0\n",
      "violation: 0.09570502792298462\n",
      "violation: 0.025688803587810886\n",
      "violation: 0.014858330941359589\n",
      "violation: 0.010081881001577999\n",
      "violation: 0.007715445880333024\n",
      "violation: 0.00644602570541173\n",
      "violation: 0.005695686132134661\n",
      "violation: 0.005204996559427115\n",
      "violation: 0.00485164971218167\n",
      "violation: 0.004576035040271899\n",
      "violation: 0.004351945152275481\n",
      "violation: 0.004156671435274464\n",
      "violation: 0.003983750521579585\n",
      "violation: 0.0038268599531295702\n",
      "violation: 0.003683016236536252\n",
      "violation: 0.0035476872688797742\n",
      "violation: 0.0034211764101554068\n",
      "violation: 0.0033046339791301007\n",
      "violation: 0.0031937923532410987\n",
      "violation: 0.0030864567262139644\n",
      "violation: 0.00298370826542638\n",
      "violation: 0.0028851866975187065\n",
      "violation: 0.002791450830746885\n",
      "violation: 0.0027028222253284756\n",
      "violation: 0.002617287679408982\n",
      "violation: 0.0025352885133932824\n",
      "violation: 0.002457314857778799\n",
      "violation: 0.0023808541827562606\n",
      "violation: 0.002308275513476838\n",
      "violation: 0.00223988356623764\n",
      "violation: 0.0021718709037046244\n",
      "violation: 0.00210590675582267\n",
      "violation: 0.0020434876066403596\n",
      "violation: 0.001983943819022395\n",
      "violation: 0.0019260892556519126\n",
      "violation: 0.0018687618023220803\n",
      "violation: 0.001813731491362421\n",
      "violation: 0.001761748086226282\n",
      "violation: 0.0017107478737251908\n",
      "violation: 0.0016607100725564196\n",
      "violation: 0.0016118744229312507\n",
      "violation: 0.0015657345478963056\n",
      "violation: 0.0015221561055284685\n",
      "violation: 0.001478794976820473\n",
      "violation: 0.0014360177610477638\n",
      "violation: 0.0013945733571696366\n",
      "violation: 0.0013548888494224554\n",
      "violation: 0.0013176278930484382\n",
      "violation: 0.001281941834144841\n",
      "violation: 0.0012469122644566913\n",
      "violation: 0.0012126064176251684\n",
      "violation: 0.0011788104110114846\n",
      "violation: 0.0011467820474267323\n",
      "violation: 0.0011154389408760588\n",
      "violation: 0.00108526504099116\n",
      "violation: 0.0010563215619201899\n",
      "violation: 0.0010276077101766248\n",
      "violation: 0.0010003758828251352\n",
      "violation: 0.0009735485429492934\n",
      "violation: 0.0009474949976676693\n",
      "violation: 0.000922260430205337\n",
      "violation: 0.0008981055183557628\n",
      "violation: 0.0008743165947395652\n",
      "violation: 0.0008510856411947798\n",
      "violation: 0.0008287472612589533\n",
      "violation: 0.0008072814416589392\n",
      "violation: 0.0007865109932279365\n",
      "violation: 0.0007660386902668454\n",
      "violation: 0.0007464825578535351\n",
      "violation: 0.0007275579960479415\n",
      "violation: 0.000708814825215122\n",
      "violation: 0.0006909219663735637\n",
      "violation: 0.0006734759929740982\n",
      "violation: 0.0006564612275960093\n",
      "violation: 0.0006398101018183634\n",
      "violation: 0.0006237744696565219\n",
      "violation: 0.0006079895734563063\n",
      "violation: 0.0005925310298609065\n",
      "violation: 0.0005778841520262766\n",
      "violation: 0.0005637155809440884\n",
      "violation: 0.0005500886048858251\n",
      "violation: 0.0005365326027718221\n",
      "violation: 0.0005235659017885785\n",
      "violation: 0.000510913006269843\n",
      "violation: 0.0004984934491974962\n",
      "violation: 0.0004860502505616844\n",
      "violation: 0.0004740238966521218\n",
      "violation: 0.0004624042508345518\n",
      "violation: 0.0004512332214575095\n",
      "violation: 0.0004404478648842747\n",
      "violation: 0.0004300593124811437\n",
      "violation: 0.0004198846481370097\n",
      "violation: 0.0004101941019028386\n",
      "violation: 0.00040054251174770507\n",
      "violation: 0.00039109239212614\n",
      "violation: 0.0003819711488157127\n",
      "violation: 0.00037312105448521096\n",
      "violation: 0.00036444227479875507\n",
      "violation: 0.0003560553161227943\n",
      "violation: 0.0003477327978440947\n",
      "violation: 0.00033976048186954545\n",
      "violation: 0.0003319979978366834\n",
      "violation: 0.0003245147388811154\n",
      "violation: 0.00031712108040283057\n",
      "violation: 0.0003098481014273375\n",
      "violation: 0.00030281813376697444\n",
      "violation: 0.0002960090885222554\n",
      "violation: 0.00028932952927163174\n",
      "violation: 0.0002827366964113659\n",
      "violation: 0.0002762369671066297\n",
      "violation: 0.00026989484856508177\n",
      "violation: 0.0002637456746959069\n",
      "violation: 0.0002578829403062226\n",
      "violation: 0.00025204303522123107\n",
      "violation: 0.000246302907517423\n",
      "violation: 0.0002407155369575284\n",
      "violation: 0.00023530292861604455\n",
      "violation: 0.00023006352101132504\n",
      "violation: 0.0002249447272829615\n",
      "violation: 0.00021986339408702128\n",
      "violation: 0.00021502754983933033\n",
      "violation: 0.00021032829376706945\n",
      "violation: 0.00020569520435673421\n",
      "violation: 0.000201157821582612\n",
      "violation: 0.00019673549917637978\n",
      "violation: 0.00019244513735469299\n",
      "violation: 0.00018826828969442376\n",
      "violation: 0.00018414613325725128\n",
      "violation: 0.00018009310831832025\n",
      "violation: 0.0001761698939481702\n",
      "violation: 0.00017236579871083625\n",
      "violation: 0.00016858247148411522\n",
      "violation: 0.00016483663460001847\n",
      "violation: 0.00016121986935925504\n",
      "violation: 0.0001576962940863338\n",
      "violation: 0.00015426388449713392\n",
      "violation: 0.00015091850902492786\n",
      "violation: 0.0001476549183481122\n",
      "violation: 0.00014446812524961846\n",
      "violation: 0.00014136561876992612\n",
      "violation: 0.00013831981063965108\n",
      "violation: 0.0001353013376443739\n",
      "violation: 0.0001324358566681525\n",
      "violation: 0.00012960839570147736\n",
      "violation: 0.00012685903775398028\n",
      "violation: 0.0001241420406804563\n",
      "violation: 0.00012150405107336102\n",
      "violation: 0.00011897441383896775\n",
      "violation: 0.00011647463124341868\n",
      "violation: 0.00011400223260266769\n",
      "violation: 0.00011159418118236935\n",
      "violation: 0.00010924638415066845\n",
      "violation: 0.00010696622555975244\n",
      "violation: 0.00010470204395193968\n",
      "violation: 0.00010248967899121681\n",
      "violation: 0.00010035224241680499\n",
      "violation: 9.826661934798385e-05\n",
      "Converged at iteration 158\n",
      "CPU times: user 10min 59s, sys: 11.7 s, total: 11min 11s\n",
      "Wall time: 5min 53s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.74152"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "nmf = make_pipeline(NMF(n_components=100,\n",
    "                        verbose=2,\n",
    "                       ),\n",
    "                    LogisticRegression(dual=True, solver='liblinear', max_iter=500),\n",
    "                   )\n",
    "\n",
    "pipe = nmf\n",
    "\n",
    "pipe.fit(doc_term_mat, reviews_full.train.y.items)\n",
    "(pipe.predict(valid_doc_term) == reviews_full.valid.y.items).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TruncatedSVD` is working much better.  It matches the performance of plain LR at about 500 topics, and starts to beat raw LR a bit as it goes over that.  Since the vocabulary is about 38,000 words, that's a very sizable reduction.\n",
    "\n",
    "It's also **MUCH** faster than `NMF`.  I only tried to run up to about 100 components there, where it was starting to get too slow for my lunch break."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 38438)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_term_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 26s, sys: 20 s, total: 4min 46s\n",
      "Wall time: 3min 1s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.86992"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "nmf = make_pipeline(TruncatedSVD(n_components=2000,\n",
    "                       ),\n",
    "                    LogisticRegression(dual=True, solver='liblinear', max_iter=500),\n",
    "                   )\n",
    "\n",
    "pipe = nmf\n",
    "\n",
    "pipe.fit(doc_term_mat, reviews_full.train.y.items)\n",
    "(pipe.predict(valid_doc_term) == reviews_full.valid.y.items).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 57s, sys: 13.4 s, total: 3min 11s\n",
      "Wall time: 2min 7s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8734"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "nmf = make_pipeline(TruncatedSVD(n_components=1500,\n",
    "                       ),\n",
    "                    LogisticRegression(dual=True, solver='liblinear', max_iter=500),\n",
    "                   )\n",
    "\n",
    "pipe = nmf\n",
    "\n",
    "pipe.fit(doc_term_mat, reviews_full.train.y.items)\n",
    "(pipe.predict(valid_doc_term) == reviews_full.valid.y.items).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/anaconda3/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 49s, sys: 4.28 s, total: 1min 53s\n",
      "Wall time: 1min 16s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.86664"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "nmf = make_pipeline(TruncatedSVD(n_components=1000,\n",
    "                       ),\n",
    "                    LogisticRegression(dual=True, solver='liblinear', max_iter=500),\n",
    "                   )\n",
    "\n",
    "pipe = nmf\n",
    "\n",
    "pipe.fit(doc_term_mat, reviews_full.train.y.items)\n",
    "(pipe.predict(valid_doc_term) == reviews_full.valid.y.items).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying the pipeline with TF-IDF\n",
    "\n",
    "TF-IDF is working much better than the `CountVectorizer`.\n",
    "\n",
    "It's much harder to beat the default performance (according to accuracy) with TF-IDF, but it does evenrually happen for `TruncatedSVD` after about 3000 topics.\n",
    "\n",
    "I should look more into Precision and Recall, see what those metrics look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_docs = [x.text for x in reviews_full.train.x]\n",
    "valid_docs = [x.text for x in reviews_full.valid.x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, with no topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.87 s, sys: 46.5 ms, total: 7.91 s\n",
      "Wall time: 7.92 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.88144"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "nmf = make_pipeline(sklearn_text.TfidfVectorizer(),\n",
    "                    LogisticRegression(dual=True, solver='liblinear', max_iter=500),\n",
    "                   )\n",
    "\n",
    "pipe = nmf\n",
    "\n",
    "pipe.fit(train_docs, reviews_full.train.y.items)\n",
    "(pipe.predict(valid_docs) == reviews_full.valid.y.items).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 27s, sys: 5.19 s, total: 1min 32s\n",
      "Wall time: 55.8 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.87816"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "nmf = make_pipeline(sklearn_text.TfidfVectorizer(),\n",
    "                    TruncatedSVD(n_components=1000,\n",
    "                       ),\n",
    "                    LogisticRegression(dual=True, solver='liblinear', max_iter=500),\n",
    "                   )\n",
    "\n",
    "pipe = nmf\n",
    "\n",
    "pipe.fit(train_docs, reviews_full.train.y.items)\n",
    "(pipe.predict(valid_docs) == reviews_full.valid.y.items).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 24s, sys: 9.58 s, total: 2min 33s\n",
      "Wall time: 1min 30s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.87976"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "nmf = make_pipeline(sklearn_text.TfidfVectorizer(),\n",
    "                    TruncatedSVD(n_components=1500,\n",
    "                       ),\n",
    "                    LogisticRegression(dual=True, solver='liblinear', max_iter=500),\n",
    "                   )\n",
    "\n",
    "pipe = nmf\n",
    "\n",
    "pipe.fit(train_docs, reviews_full.train.y.items)\n",
    "(pipe.predict(valid_docs) == reviews_full.valid.y.items).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 1s, sys: 22.8 s, total: 4min 24s\n",
      "Wall time: 2min 20s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.88128"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "nmf = make_pipeline(sklearn_text.TfidfVectorizer(),\n",
    "                    TruncatedSVD(n_components=2000,\n",
    "                       ),\n",
    "                    LogisticRegression(dual=True, solver='liblinear', max_iter=500),\n",
    "                   )\n",
    "\n",
    "pipe = nmf\n",
    "\n",
    "pipe.fit(train_docs, reviews_full.train.y.items)\n",
    "(pipe.predict(valid_docs) == reviews_full.valid.y.items).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 45s, sys: 29.6 s, total: 7min 14s\n",
      "Wall time: 3min 40s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8818"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "nmf = make_pipeline(sklearn_text.TfidfVectorizer(),\n",
    "                    TruncatedSVD(n_components=3000,\n",
    "                       ),\n",
    "                    LogisticRegression(dual=True, solver='liblinear', max_iter=500),\n",
    "                   )\n",
    "\n",
    "pipe = nmf\n",
    "\n",
    "pipe.fit(train_docs, reviews_full.train.y.items)\n",
    "(pipe.predict(valid_docs) == reviews_full.valid.y.items).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
